{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deepDRCNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"lGpOzxbCPt9w","colab_type":"code","colab":{}},"source":["seed = 5 \n","import numpy as np \n","np.random.seed(seed)\n","import tensorflow as tf\n","tf.set_random_seed(seed)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6vdpK0sKQknk","colab_type":"text"},"source":["Dataset Loading"]},{"cell_type":"code","metadata":{"id":"tpbH1GXnQmC1","colab_type":"code","colab":{}},"source":["import json\n","import math\n","import os\n","\n","import cv2\n","from PIL import Image\n","\n","from keras import layers\n","from keras.applications.resnet50 import ResNet50, preprocess_input\n","from keras.applications import DenseNet121\n","from keras.callbacks import Callback, ModelCheckpoint,EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import cohen_kappa_score, accuracy_score\n","import scipy\n","from tqdm import tqdm\n","print(os.listdir('../input/aptos2019-blindness-detection'))\n","%matplotlib inline\n","\n","IMG_SIZE=256\n","BATCH_SIZE = 32"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AEDPc_yCQvTT","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"lRjqFM9SQz8s","colab_type":"code","colab":{}},"source":["train_df = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n","test_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')\n","\n","\n","train_df.head()\n","train_df['diagnosis'].value_counts()\n","\n","\n","\n","train_df['diagnosis'].hist()\n","train_df['diagnosis'].value_counts()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qpIznTmRFZz","colab_type":"text"},"source":["Preprocessing & Augmentation"]},{"cell_type":"code","metadata":{"id":"7hGxpB5uRIPb","colab_type":"code","colab":{}},"source":["def get_pad_width(im, new_shape, is_rgb=True):\n","    pad_diff = new_shape - im.shape[0], new_shape - im.shape[1]\n","    t, b = math.floor(pad_diff[0]/2), math.ceil(pad_diff[0]/2)\n","    l, r = math.floor(pad_diff[1]/2), math.ceil(pad_diff[1]/2)\n","    if is_rgb:\n","        pad_width = ((t,b), (l,r), (0, 0))\n","    else:\n","        pad_width = ((t,b), (l,r))\n","    return pad_width\n","\n","def crop_image_from_gray(img,tol=7):\n","    if img.ndim ==2:\n","        mask = img>tol\n","        return img[np.ix_(mask.any(1),mask.any(0))]\n","    elif img.ndim==3:\n","        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n","        mask = gray_img>tol\n","        \n","        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n","        if (check_shape == 0): # image is too dark so that we crop out everything,\n","            return img # return original image\n","        else:\n","            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n","            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n","            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n","    #         print(img1.shape,img2.shape,img3.shape)\n","            img = np.stack([img1,img2,img3],axis=-1)\n","    #         print(img.shape)\n","        return img\n","    \n","def preprocess_image(path, sigmaX=40):\n","    image = cv2.imread(path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    image = crop_image_from_gray(image)\n","    image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n","#     image=cv2.addWeighted ( image,4, cv2.GaussianBlur( image , (0,0) , sigmaX) ,-4 ,128)\n","        \n","    return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"At9joW4GRJzK","colab_type":"code","colab":{}},"source":["N = train_df.shape[0]\n","x_train = np.empty((N, IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n","\n","for i, image_id in enumerate(tqdm(train_df['id_code'])):\n","    x_train[i, :, :, :] = preprocess_image(\n","        f'../input/aptos2019-blindness-detection/train_images/{image_id}.png'\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wCYAO2anRXA8","colab_type":"code","colab":{}},"source":["\n","\n","y_train = pd.get_dummies(train_df['diagnosis']).values\n","\n","\n","print(y_train.shape)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F1tVC7BDRZrm","colab_type":"text"},"source":["Multilabeling"]},{"cell_type":"code","metadata":{"id":"S6HLOUaDRbOz","colab_type":"code","colab":{}},"source":["\n","\n","y_train_multi = np.empty(y_train.shape, dtype=y_train.dtype)\n","y_train_multi[:, 4] = y_train[:, 4]\n","\n","for i in range(3, -1, -1):\n","    y_train_multi[:, i] = np.logical_or(y_train[:, i], y_train_multi[:, i+1])\n","\n","print(\"Original y_train:\", y_train.sum(axis=0))\n","print(\"Multilabel version:\", y_train_multi.sum(axis=0))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rcXS4MksReWl","colab_type":"text"},"source":["Dataset Spliting for Training & Validation"]},{"cell_type":"code","metadata":{"id":"b7LtpRlZRjLC","colab_type":"code","colab":{}},"source":["\n","\n","x_train, x_val, y_train, y_val = train_test_split(\n","    x_train, y_train_multi, \n","    test_size=0.15, \n","    random_state=2019\n",")\n","\n","\n","\n","x_val=x_val/255\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZTu_MBZrRlWP","colab_type":"text"},"source":["Augmentation"]},{"cell_type":"code","metadata":{"id":"TOIgXN1HRrsz","colab_type":"code","colab":{}},"source":["class MixupGenerator():\n","    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        self.batch_size = batch_size\n","        self.alpha = alpha\n","        self.shuffle = shuffle\n","        self.sample_num = len(X_train)\n","        self.datagen = datagen\n","\n","    def __call__(self):\n","        while True:\n","            indexes = self.__get_exploration_order()\n","            itr_num = int(len(indexes) // (self.batch_size * 2))\n","\n","            for i in range(itr_num):\n","                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n","                X, y = self.__data_generation(batch_ids)\n","\n","                yield X, y\n","\n","    def __get_exploration_order(self):\n","        indexes = np.arange(self.sample_num)\n","\n","        if self.shuffle:\n","            np.random.shuffle(indexes)\n","\n","        return indexes\n","\n","    def __data_generation(self, batch_ids):\n","        _, h, w, c = self.X_train.shape\n","        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n","        X_l = l.reshape(self.batch_size, 1, 1, 1)\n","        y_l = l.reshape(self.batch_size, 1)\n","\n","        X1 = self.X_train[batch_ids[:self.batch_size]]\n","        X2 = self.X_train[batch_ids[self.batch_size:]]\n","        X = X1 * X_l + X2 * (1 - X_l)\n","\n","        if self.datagen:\n","            for i in range(self.batch_size):\n","                X[i] = self.datagen.random_transform(X[i])\n","                X[i] = self.datagen.standardize(X[i])\n","\n","        if isinstance(self.y_train, list):\n","            y = []\n","\n","            for y_train_ in self.y_train:\n","                y1 = y_train_[batch_ids[:self.batch_size]]\n","                y2 = y_train_[batch_ids[self.batch_size:]]\n","                y.append(y1 * y_l + y2 * (1 - y_l))\n","        else:\n","            y1 = self.y_train[batch_ids[:self.batch_size]]\n","            y2 = self.y_train[batch_ids[self.batch_size:]]\n","            y = y1 * y_l + y2 * (1 - y_l)\n","\n","        return X, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHFee-Q1RuyL","colab_type":"code","colab":{}},"source":["def create_datagen():\n","    return ImageDataGenerator(\n","        zoom_range=0.15,  # set range for random zoom\n","        # set mode for filling points outside the input boundaries\n","        fill_mode='constant',\n","        cval=0.,  # value used for fill_mode = \"constant\"\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=True,# randomly flip images\n","        rotation_range=360,\n","        rescale=1./255\n","    )\n","  \n","  data_generator = create_datagen().flow(x_train, y_train, batch_size=BATCH_SIZE, seed=2019)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e558qHA7R_hC","colab_type":"text"},"source":["QWK"]},{"cell_type":"code","metadata":{"id":"_AZI20VTSAZy","colab_type":"code","colab":{}},"source":["class Metrics(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.val_kappas = []\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","# #         X_val, y_val =self.val_generator[0],self.val_generator[1]\n","# #         y_val = y_val.sum(axis=1) - 1\n","#         y_pred=self.model.predict_generator(val_generator,steps=np.ceil(val_generator.samples/BATCH_SIZE).astype(int))\n","# #         y_pred = self.model.predict(X_val) > 0.5\n","# #         y_pred = y_pred.astype(int).sum(axis=1) - 1\n","#         y_pred=np.argmax(y_pred,axis=1)\n","\n","    \n","        X_val, y_val = self.validation_data[:2]\n","        y_val = y_val.sum(axis=1) - 1\n","        \n","        y_pred = self.model.predict(X_val) > 0.5\n","        y_pred = y_pred.astype(int).sum(axis=1) - 1\n","        \n","        _val_kappa = cohen_kappa_score(\n","            y_val,\n","            y_pred, \n","            weights='quadratic'\n","        )\n","\n","        self.val_kappas.append(_val_kappa)\n","\n","        print(f\"val_kappa: {_val_kappa:.4f}\")\n","        \n","        if _val_kappa == max(self.val_kappas):\n","            print(\"Validation Kappa has improved. Saving model.\")\n","            model.save_weights('model.h5')\n","            model_json = model.to_json()\n","            with open('model.json', \"w\") as json_file:\n","                json_file.write(model_json)\n","            json_file.close()\n","\n","        return"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gs7F0HXDSGuz","colab_type":"text"},"source":["DenseNet Model"]},{"cell_type":"code","metadata":{"id":"7tM1RcSFSLCi","colab_type":"code","colab":{}},"source":["\n","\n","densenet = DenseNet121(\n","    weights='../input/densenet-keras/DenseNet-BC-121-32-no-top.h5',\n","    include_top=False,\n","    input_shape=(IMG_SIZE,IMG_SIZE,3)\n",")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RTWrBmjeSOGE","colab_type":"code","colab":{}},"source":["def build_model():\n","    model = Sequential()\n","    model.add(densenet)\n","    model.add(layers.GlobalAveragePooling2D())\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(5, activation='sigmoid'))\n","    \n","#     for layer in model.layers:\n","#         layer.trainable=True\n","    \n","    model.compile(\n","        loss='binary_crossentropy',\n","        optimizer=Adam(lr=0.00005),\n","        metrics=['accuracy']\n","    )\n","    \n","    return model\n","  \n","  \n","  model = build_model()\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9SS4e2ORSVrD","colab_type":"text"},"source":["Training & Resul"]},{"cell_type":"code","metadata":{"id":"DjSUknzmSXBC","colab_type":"code","colab":{}},"source":["kappa_metrics = Metrics()\n","# filename='1500images_lr003.h5'\n","# checkpoint=ModelCheckpoint(filename,monitor='val_loss',save_best_only='True')\n","est=EarlyStopping(monitor='val_loss',patience=5, min_delta=0.005)\n","call_backs=[est,kappa_metrics]\n","\n","history = model.fit_generator(\n","    data_generator,\n","    steps_per_epoch=x_train.shape[0] / BATCH_SIZE,\n","    validation_data=(x_val,y_val),\n","     epochs=20,\n","    callbacks=call_backs)"],"execution_count":0,"outputs":[]}]}